{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from transformers import GPT2Model, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(p):\n",
    "  if np.sum(p) != 1:\n",
    "    p = p / p.sum()\n",
    "  return np.random.choice(range(len(p)), p=p)\n",
    "\n",
    "class GPT2:\n",
    "\n",
    "  hparams_dict = {\n",
    "    \"gpt2\":        dict(n_layer=12, n_head=12, n_embed=768),   # 124M params\n",
    "    \"gpt2-medium\": dict(n_layer=24, n_head=16, n_embed=1024),  # 350M params\n",
    "    \"gpt2-large\":  dict(n_layer=36, n_head=20, n_embed=1280),  # 774M params\n",
    "    \"gpt2-xl\":     dict(n_layer=48, n_head=25, n_embed=1600),  # 1558M params\n",
    "  }\n",
    "  \n",
    "  context_len = 1024\n",
    "\n",
    "  def __init__(self, model_type):\n",
    "    self.hparams = self.hparams_dict[model_type]\n",
    "    self.ws = {k: v.numpy() for k, v in GPT2Model.from_pretrained(model_type).state_dict().items()}\n",
    "\n",
    "  def generate(self, start_ids, max_new_tokens, temperature=1.0, draft_model=None, K=4, stream=True, stream_printer=None):\n",
    "    ret_p, ret_ids = [], []\n",
    "    cnt = 0\n",
    "    if stream_printer is not None: stream_printer(start_ids)\n",
    "      \n",
    "    while cnt < max_new_tokens:\n",
    "      ids_cond = (start_ids + ret_ids)[-self.context_len:]\n",
    "\n",
    "      if draft_model is not None:  # speculative_sampling\n",
    "        # 1. sample K steps from draft model\n",
    "        p_draft, ids_draft = draft_model.generate(ids_cond, K, temperature=temperature)\n",
    "        # 2. forward target model\n",
    "        p = self.forward(ids_cond + ids_draft)[-K-1:]\n",
    "        # 3. loop throught draft tokens and perform reject samping\n",
    "        new_p, new_ids = [], []\n",
    "        all_accepted = True\n",
    "        for i in range(K):\n",
    "          j = ids_draft[i]\n",
    "          if np.random.uniform() >= min(1, p[i][j]/p_draft[i][j]):\n",
    "            # if current draft token j is rejected, we resample a token from normalized max(0, p-q)\n",
    "            new_ids.append(sample(np.maximum(p[i] - p_draft[i], 0)))\n",
    "            new_p.append(p[i])\n",
    "            all_accepted = False\n",
    "            break\n",
    "          new_ids.append(j)\n",
    "          new_p.append(p[i])\n",
    "        if all_accepted:\n",
    "          # sample extra token x_{n+k+1} if all draft tokens were accepted\n",
    "          new_ids.append(sample(p[-1]))\n",
    "          new_p.append(p[-1])\n",
    "      else:\n",
    "        # autoregressive sampling\n",
    "        p = self.forward(ids_cond)[-1]\n",
    "        new_p, new_ids = [p], [sample(p)]\n",
    "\n",
    "      ret_p += new_p\n",
    "      ret_ids += new_ids\n",
    "      cnt += len(new_ids)\n",
    "      if stream_printer is not None: stream_printer(new_ids)\n",
    "        \n",
    "    if stream_printer is not None: print()\n",
    "    return np.vstack(ret_p), ret_ids\n",
    "\n",
    "  def forward(self, ids, only_last=True):\n",
    "    \"\"\"minimal numpy implementation of gpt2 forward pass\"\"\"\n",
    "    ws, hparams = self.ws, self.hparams\n",
    "\n",
    "    def layer_norm(x, w, b, eps=1e-5):\n",
    "      mean = np.mean(x, axis=-1, keepdims=True)\n",
    "      var = np.var(x, axis=-1, keepdims=True)\n",
    "      return ((x - mean) / (var + eps)**0.5) * w + b\n",
    "\n",
    "    def softmax(x, axis=-1):\n",
    "      x -= x.max(axis=axis, keepdims=True)\n",
    "      x = np.exp(x, x)\n",
    "      x /= x.sum(axis=axis, keepdims=True)\n",
    "      return x\n",
    "\n",
    "    def transformer_block(x, i):\n",
    "\n",
    "      def linear(x, w, b):\n",
    "        return x @ w + b\n",
    "\n",
    "      def gelu(x):\n",
    "        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "      def mha(x, i):\n",
    "        T, C = x.shape\n",
    "        x = linear(x, ws[f\"h.{i}.attn.c_attn.weight\"], ws[f\"h.{i}.attn.c_attn.bias\"])\n",
    "        n_head, hs = hparams[\"n_head\"], C // hparams[\"n_head\"]\n",
    "        q, k, v = [np.transpose(h.reshape((T, n_head, hs)), (1,0,2)) for h in np.split(x, 3, axis=-1)]\n",
    "        attn = softmax(q @ np.transpose(k, (0,2,1)) / hs**0.5 + (1 - np.tri(T, dtype=np.float32)) * -1e10)\n",
    "        x = np.transpose(attn @ v, (1,0,2)).reshape((T, C))\n",
    "        x = linear(x, ws[f\"h.{i}.attn.c_proj.weight\"], ws[f\"h.{i}.attn.c_proj.bias\"])\n",
    "        return x\n",
    "\n",
    "      def mlp(x, i):\n",
    "        x = gelu(linear(x, ws[f\"h.{i}.mlp.c_fc.weight\"], ws[f\"h.{i}.mlp.c_fc.bias\"]))\n",
    "        x = linear(x, ws[f\"h.{i}.mlp.c_proj.weight\"], ws[f\"h.{i}.mlp.c_proj.bias\"])\n",
    "        return x\n",
    "\n",
    "      x = x + mha(layer_norm(x, ws[f\"h.{i}.ln_1.weight\"], ws[f\"h.{i}.ln_1.bias\"]), i)\n",
    "      x = x + mlp(layer_norm(x, ws[f\"h.{i}.ln_2.weight\"], ws[f\"h.{i}.ln_2.bias\"]), i)\n",
    "      return x\n",
    "\n",
    "    wte, wpe = ws[\"wte.weight\"], ws[\"wpe.weight\"]\n",
    "    x = wte[ids] + wpe[range(len(ids))]\n",
    "    for i in range(hparams[\"n_layer\"]):\n",
    "      x = transformer_block(x, i)\n",
    "    x = layer_norm(x, ws[\"ln_f.weight\"], ws[\"ln_f.bias\"])\n",
    "    logits = (x @ wte.T) / (temperature + 1e-8)\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "target_model_name = \"gpt2-xl\"\n",
    "draft_model_name = \"gpt2\"\n",
    "max_new_tokens = 50\n",
    "temperature = 0  # large temperature -> more random, 0 -> greedy\n",
    "K = 4\n",
    "prompt = \"Alan Turing theorized that computers would one day become\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "draft_model = GPT2(draft_model_name)\n",
    "target_model = GPT2(target_model_name)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(target_model_name)\n",
    "start_ids = tokenizer(prompt)[\"input_ids\"]\n",
    "\n",
    "def stream_printer(ids):\n",
    "  print(tokenizer.decode(ids), end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular autoregressive sampling\n",
      "--------------------------------------------------\n",
      "Alan Turing theorized that computers would one day become so powerful that they would be able to think like humans.\n",
      "\n",
      "In the 1950s, he proposed a way to build a computer that could think like a human. He called it the \"Turing machine.\"\n",
      "\n",
      "The machine was a mechanical\n",
      "--------------------------------------------------\n",
      "cost: 53.53s, 0.93 tokens/s\n"
     ]
    }
   ],
   "source": [
    "print(\"regular autoregressive sampling\")\n",
    "print(\"-\"*50)\n",
    "st = time.monotonic()\n",
    "_, ids = target_model.generate(start_ids, max_new_tokens, temperature, stream_printer=stream_printer)\n",
    "cost = time.monotonic() - st\n",
    "print(\"-\"*50)\n",
    "print(f\"cost: {cost:.2f}s, {len(ids)/cost:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speculative sampling\n",
      "--------------------------------------------------\n",
      "Alan Turing theorized that computers would one day become so powerful that they would be able to think like humans.\n",
      "\n",
      "In the 1950s, he proposed a way to build a computer that could think like a human. He called it the \"Turing machine.\"\n",
      "\n",
      "The machine was a mechanical\n",
      "--------------------------------------------------\n",
      "cost: 31.53s, 1.59 tokens/s\n"
     ]
    }
   ],
   "source": [
    "print(\"speculative sampling\")\n",
    "print(\"-\"*50)\n",
    "st = time.monotonic()\n",
    "_, ids = target_model.generate(start_ids, max_new_tokens, temperature, draft_model=draft_model, K=K, stream_printer=stream_printer)\n",
    "cost = time.monotonic() - st\n",
    "print(\"-\"*50)\n",
    "print(f\"cost: {cost:.2f}s, {len(ids)/cost:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "nanogpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
